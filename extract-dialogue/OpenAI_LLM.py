import openai
import os
from langchain.llms.base import LLM
from typing import Dict, List, Optional, Tuple, Union
from dotenv import find_dotenv, load_dotenv
from openai import OpenAI

_ = load_dotenv(find_dotenv())


def get_completion(prompt, model="gpt-3.5-turbo"):
    '''
    prompt: 对应的提示词
    model: 调用的模型，默认为 gpt-3.5-turbo(ChatGPT)，有内测资格的用户可以选择 gpt-4
    '''
    # messages = [{"role": "user", "content": prompt}]
    # response = openai.ChatCompletion.create(
    #     model=model,
    #     messages=messages,
    #     temperature=0, # 模型输出的温度系数，控制输出的随机程度
    # )
    # 调用 OpenAI 的 ChatCompletion 接口
    client = OpenAI(
                api_key='',
                # base_url='https://api.moonshot.cn/v1',
                base_url='',
            )

    messages = [{
        'role':
        'system',
        'content':
        prompt,
    }]

    # messages.append({'role': 'user', 'content': prompt})


    response = client.chat.completions.create(
        model='gpt-3.5-turbo',
        messages=messages,
        temperature=0.,
    )
    return response.choices[0].message.content

class OpenAI_LLM(LLM):
    model_type: str = "openai"


    def __init__(self):
        super().__init__()

    @property
    def _llm_type(self) -> str:
        return "openai"
    
    def _call(self, prompt: str, history: List = [], stop: Optional[List[str]] = None) -> str:
        res = get_completion(prompt)
        return res
    
    @property
    def _identifying_params(self):
        """Get the identifying parameters.
        """
        _param_dict = {
            "model": self.model_type
        }
        return _param_dict

# llm = OpenAI_LLM()
# print(llm('你好'))